{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "6156d543473c856a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:09:46.242131Z",
     "start_time": "2025-11-30T16:09:45.733103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_trf import load_imdb, load_imdb_synth, load_xor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clara/Desktop/VU/Year 2/P2/Deep Learning/Assignments/Assignment 3/transformer/.dl_venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading and Inspecting the data",
   "id": "1630dc32bdfc06a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:10:38.588137Z",
     "start_time": "2025-11-30T16:10:38.239859Z"
    }
   },
   "cell_type": "code",
   "source": "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)",
   "id": "6abd5be25bccccb3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T13:49:20.136085Z",
     "start_time": "2025-11-30T13:49:20.132963Z"
    }
   },
   "cell_type": "code",
   "source": "print(type(x_train), type(y_train), type(i2w), type(w2i))",
   "id": "81195b189bb96853",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'list'> <class 'dict'>\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T13:51:24.904209Z",
     "start_time": "2025-11-30T13:51:24.899497Z"
    }
   },
   "cell_type": "code",
   "source": "x_train[141]",
   "id": "abbdae617c44286c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[880,\n",
       " 4,\n",
       " 122,\n",
       " 19,\n",
       " 127,\n",
       " 1035,\n",
       " 12,\n",
       " 4,\n",
       " 481,\n",
       " 7,\n",
       " 4372,\n",
       " 7913,\n",
       " 618,\n",
       " 5,\n",
       " 32,\n",
       " 15,\n",
       " 146,\n",
       " 33,\n",
       " 585,\n",
       " 678,\n",
       " 37,\n",
       " 34,\n",
       " 623,\n",
       " 12,\n",
       " 348,\n",
       " 20,\n",
       " 30,\n",
       " 14253,\n",
       " 3414,\n",
       " 5,\n",
       " 13112,\n",
       " 28,\n",
       " 969,\n",
       " 147,\n",
       " 30,\n",
       " 105,\n",
       " 5,\n",
       " 30,\n",
       " 1816,\n",
       " 7,\n",
       " 118]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T13:49:52.320615Z",
     "start_time": "2025-11-30T13:49:52.318011Z"
    }
   },
   "cell_type": "code",
   "source": "print(i2w[141], w2i['film'])",
   "id": "b5c7dd1178b5d48c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ve 23\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T13:50:56.690205Z",
     "start_time": "2025-11-30T13:50:56.687849Z"
    }
   },
   "cell_type": "code",
   "source": "print([i2w[w] for w in x_train[141]])",
   "id": "e7da7157d3e9a368",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['possibly', 'the', 'best', 'movie', 'ever', 'created', 'in', 'the', 'history', 'of', 'jeffrey', 'combs', 'career', 'and', 'one', 'that', 'should', 'be', 'looked', 'upon', 'by', 'all', 'talent', 'in', 'hollywood', 'for', 'his', 'versatility', 'charisma', 'and', 'uniqueness', 'he', 'brings', 'through', 'his', 'characters', 'and', 'his', 'knowledge', 'of', 'acting']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:12:11.824835Z",
     "start_time": "2025-11-30T14:12:11.822616Z"
    }
   },
   "cell_type": "code",
   "source": "w2i['.pad']",
   "id": "d941244e9205c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 1:  Padding & Converting to torch tensors",
   "id": "f92a65d5d9dfca4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:10:44.706344Z",
     "start_time": "2025-11-30T16:10:42.359544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_and_convert(seqs, pad_idx):\n",
    "    # 1. Find max sequence length\n",
    "    max_len = max(len(s) for s in seqs)\n",
    "\n",
    "    padded = []\n",
    "    for s in seqs:\n",
    "        # 2. Pad with pad_idx to max_len\n",
    "        padded_seq = s + [pad_idx] * (max_len - len(s))\n",
    "        padded.append(padded_seq)\n",
    "\n",
    "    # 3. Convert to tensor (long dtype)\n",
    "    return torch.tensor(padded, dtype=torch.long)\n",
    "\n",
    "\n",
    "def prepare_data(x_train, y_train, x_val, y_val, w2i):\n",
    "    pad_idx = w2i[\".pad\"]\n",
    "\n",
    "    x_train_t = pad_and_convert(x_train, pad_idx)\n",
    "    x_val_t   = pad_and_convert(x_val, pad_idx)\n",
    "\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val_t   = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    return x_train_t, y_train_t, x_val_t, y_val_t\n",
    "\n",
    "x_train_t, y_train_t, x_val_t, y_val_t = prepare_data(x_train, y_train, x_val, y_val, w2i)\n",
    "\n",
    "print(x_train_t.shape)\n",
    "print(y_train_t.shape)"
   ],
   "id": "c5f34b2c61dcd95f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 2514])\n",
      "torch.Size([20000])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:12:47.070148Z",
     "start_time": "2025-11-30T14:12:47.065664Z"
    }
   },
   "cell_type": "code",
   "source": "x_train_t[2]",
   "id": "db3afd116b498f08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10721,     4, 10956,  ...,     0,     0,     0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:14:19.511864Z",
     "start_time": "2025-11-30T14:14:19.508736Z"
    }
   },
   "cell_type": "code",
   "source": "y_train_t[2]",
   "id": "d233df8c6d50ae38",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 2: Building Model",
   "id": "b3759b7cd5bde524"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:46:16.244526Z",
     "start_time": "2025-11-30T14:46:15.967662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaselineClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, time), dtype long\n",
    "        \"\"\"\n",
    "        # 1. Embedding\n",
    "        emb = self.emb(x)  # → (batch, time, emb)\n",
    "\n",
    "        # 2. Global average pool over time dimension\n",
    "        pooled = emb.mean(dim=1)  # → (batch, emb)\n",
    "\n",
    "        # 3. Linear projection to classes\n",
    "        logits = self.classifier(pooled)  # → (batch, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = BaselineClassifier(\n",
    "    vocab_size=len(w2i),\n",
    "    emb_dim=300,\n",
    "    num_classes=numcls\n",
    ")\n",
    "\n",
    "# logits = model(x_train_t[:32])\n",
    "# print(logits.shape)"
   ],
   "id": "a4b3fa8704607e64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 3: 3 different global pools",
   "id": "b99dd0a622ac592b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:10:00.562843Z",
     "start_time": "2025-11-30T16:10:00.558560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model with 3 global pooling options\n",
    "class BaselinePool(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_classes, pool_type=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type \n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim\n",
    "        )\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x) \n",
    "\n",
    "        if self.pool_type == \"mean\":\n",
    "            pooled = emb.mean(dim=1)\n",
    "        elif self.pool_type == \"max\":\n",
    "            pooled = emb.max(dim=1).values\n",
    "        elif self.pool_type == \"first\":\n",
    "            pooled = emb[:, 0, :]\n",
    "\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ],
   "id": "d788112f297b0e26",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:10:01.569463Z",
     "start_time": "2025-11-30T16:10:01.564390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(logits, y):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in loader: # iterate through batches\n",
    "        xb, yb = xb.to(device), yb.to(device) # move batch to either CPU or GPU \n",
    "\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "\n",
    "        logits = model(xb) # feed the input batch through the model\n",
    "        loss = F.cross_entropy(logits, yb) # compute loss\n",
    "        loss.backward() # compute gradients\n",
    "        optimizer.step() # update model parameters using computed gradients \n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            total_acc += accuracy(logits, yb) * xb.size(0)\n",
    "\n",
    "    return total_acc / len(loader.dataset)\n",
    "\n",
    "def make_loaders(x_train, y_train, x_val, y_val, batch_size):\n",
    "    train_ds = TensorDataset(x_train, y_train)\n",
    "    val_ds = TensorDataset(x_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def run_experiment(dataset_name, x_train, y_train, x_val, y_val, w2i, num_classes,\n",
    "                   pool_type=\"mean\", emb_dim=300, batch_size=256, epochs=5, lr=1e-3):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    vocab_size = len(w2i)\n",
    "\n",
    "    model = BaselinePool(\n",
    "        vocab_size=vocab_size,\n",
    "        emb_dim=emb_dim,\n",
    "        num_classes=num_classes,\n",
    "        pool_type=pool_type\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    x_train_t, y_train_t, x_val_t, y_val_t = prepare_data(x_train, y_train, x_val, y_val, w2i)\n",
    "    train_loader, val_loader = make_loaders(x_train_t, y_train_t, x_val_t, y_val_t, batch_size)\n",
    "\n",
    "    print(f\"\\n=== {dataset_name.upper()} — Pool: {pool_type} ===\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "    return model\n"
   ],
   "id": "5c7a6bd3b2b4b703",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train on the imdb dataset",
   "id": "826f5d549e34b96e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T15:15:51.232587Z",
     "start_time": "2025-11-30T15:15:50.575308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "(x_tr1, y_tr1), (x_va1, y_va1), (i2w1, w2i1), num_cls1 = load_imdb()"
   ],
   "id": "3e5bcbd71cce8b26",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "=== IMDB — Pool: mean ===\n",
      "Epoch 1: train_loss=0.6946, val_acc=0.5064\n",
      "Epoch 2: train_loss=0.6930, val_acc=0.5762\n",
      "Epoch 3: train_loss=0.6887, val_acc=0.5362\n",
      "Epoch 4: train_loss=0.6801, val_acc=0.5218\n",
      "Epoch 5: train_loss=0.6720, val_acc=0.6184\n",
      "Device: cpu\n",
      "\n",
      "=== IMDB — Pool: max ===\n",
      "Epoch 1: train_loss=0.7418, val_acc=0.5320\n",
      "Epoch 2: train_loss=0.6763, val_acc=0.5950\n",
      "Epoch 3: train_loss=0.6406, val_acc=0.6584\n",
      "Epoch 4: train_loss=0.5974, val_acc=0.6288\n",
      "Epoch 5: train_loss=0.5665, val_acc=0.7454\n",
      "Device: cpu\n",
      "\n",
      "=== IMDB — Pool: first ===\n",
      "Epoch 1: train_loss=0.7096, val_acc=0.5152\n",
      "Epoch 2: train_loss=0.6858, val_acc=0.5266\n",
      "Epoch 3: train_loss=0.6754, val_acc=0.5212\n",
      "Epoch 4: train_loss=0.6662, val_acc=0.5416\n",
      "Epoch 5: train_loss=0.6582, val_acc=0.5374\n"
     ]
    }
   ],
   "execution_count": 43,
   "source": [
    "# batch_size=256, epochs=5, lr=1e-3\n",
    "for pool in [\"mean\", \"max\", \"first\"]:\n",
    "    run_experiment(\n",
    "        \"imdb\",\n",
    "        x_tr1, y_tr1,\n",
    "        x_va1, y_va1,\n",
    "        w2i1, num_cls1,\n",
    "        pool_type=pool\n",
    "    )"
   ],
   "id": "4625fefaf98f1d3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T15:35:37.163157Z",
     "start_time": "2025-11-30T15:34:12.679748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_experiment(\n",
    "        \"imdb\",\n",
    "        x_tr1, y_tr1,\n",
    "        x_va1, y_va1,\n",
    "        w2i1, num_cls1,\n",
    "        pool_type=\"max\",\n",
    "        batch_size=256,\n",
    "        lr=1e-2,\n",
    ")"
   ],
   "id": "f43819a635101563",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "=== IMDB — Pool: max ===\n",
      "Epoch 1: train_loss=1.1391, val_acc=0.7210\n",
      "Epoch 2: train_loss=0.4263, val_acc=0.7656\n",
      "Epoch 3: train_loss=0.2624, val_acc=0.8760\n",
      "Epoch 4: train_loss=0.1702, val_acc=0.8778\n",
      "Epoch 5: train_loss=0.1124, val_acc=0.8852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaselinePool(\n",
       "  (emb): Embedding(99430, 300)\n",
       "  (classifier): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c7b033773a9fa777"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train on the imdb synth dataset ",
   "id": "f6fd0275e150198f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:10:08.999001Z",
     "start_time": "2025-11-30T16:10:08.932084Z"
    }
   },
   "cell_type": "code",
   "source": "(x_tr2, y_tr2), (x_va2, y_va2), (i2w2, w2i2), num_cls2 = load_imdb_synth()",
   "id": "bfaafd297d341b70",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:12:12.332315Z",
     "start_time": "2025-11-30T16:12:12.320512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspecting data (because it threw an index out of bound error, I had to remove duplicate strings from i2w in load_imdb_synth())\n",
    "print(len(i2w2))\n",
    "print(len(w2i2))\n",
    "\n",
    "max_token = max(max(seq) for seq in x_tr2)\n",
    "vocab_size = len(w2i2)\n",
    "print(\"max token:\", max_token)\n",
    "print(\"vocab size:\", vocab_size)\n",
    "assert max_token < vocab_size, \"ERROR: token index exceeds vocabulary size!\"\n",
    "\n",
    "invalid_tokens = set(\n",
    "    idx\n",
    "    for seq in x_tr2\n",
    "    for idx in seq\n",
    "    if idx >= vocab_size\n",
    ")\n",
    "print(\"Invalid token IDs:\", invalid_tokens)\n",
    "for bad in invalid_tokens:\n",
    "    print(bad, \"→\", i2w2[bad] if bad < len(i2w2) else \"(not in i2w2)\")"
   ],
   "id": "f6912cbaea8a1a91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "69\n",
      "max token: 68\n",
      "vocab size: 69\n",
      "Invalid token IDs: set()\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:12:18.633699Z",
     "start_time": "2025-11-30T16:12:17.542128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_experiment(\"imdb_synth\",\n",
    "               x_tr2, y_tr2,\n",
    "               x_va2, y_va2,\n",
    "               w2i2, num_cls2,\n",
    "               pool_type=\"max\",\n",
    "               batch_size=256,\n",
    "               epochs=5,\n",
    "               lr=1e-2)"
   ],
   "id": "542157da9d9811ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IMDB_SYNTH — Pool: max ===\n",
      "Epoch 1: train_loss=0.5434, val_acc=1.0000\n",
      "Epoch 2: train_loss=0.0186, val_acc=1.0000\n",
      "Epoch 3: train_loss=0.0045, val_acc=1.0000\n",
      "Epoch 4: train_loss=0.0021, val_acc=1.0000\n",
      "Epoch 5: train_loss=0.0013, val_acc=1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaselinePool(\n",
       "  (emb): Embedding(69, 300)\n",
       "  (classifier): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train on the xor dataset",
   "id": "639d010220d42b14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:12:26.610595Z",
     "start_time": "2025-11-30T16:12:25.876610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(x_tr3, y_tr3), (x_va3, y_va3), (i2w3, w2i3), num_cls3 = load_xor()\n",
    "run_experiment(\n",
    "        \"xor\",\n",
    "        x_tr3, y_tr3,\n",
    "        x_va3, y_va3,\n",
    "        w2i3, num_cls3,\n",
    "        pool_type=\"max\",\n",
    "        batch_size=256,\n",
    "        lr=1e-2,\n",
    ")"
   ],
   "id": "e505346b8df1bf7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XOR — Pool: max ===\n",
      "Epoch 1: train_loss=0.0159, val_acc=1.0000\n",
      "Epoch 2: train_loss=0.0000, val_acc=1.0000\n",
      "Epoch 3: train_loss=0.0000, val_acc=1.0000\n",
      "Epoch 4: train_loss=0.0000, val_acc=1.0000\n",
      "Epoch 5: train_loss=0.0000, val_acc=1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaselinePool(\n",
       "  (emb): Embedding(6, 300)\n",
       "  (classifier): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 4: adding self attention layer",
   "id": "f08da0efad197379"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:46:28.496294Z",
     "start_time": "2025-11-30T16:46:28.490355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = (batch, time, emb)\n",
    "        # 1. Compute attention scores: (batch, time, time)\n",
    "        scores = torch.matmul(x, x.transpose(1, 2))\n",
    "\n",
    "        # 2. Softmax over the time dimension\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 3. Weighted sum of values\n",
    "        out = torch.matmul(attn, x)  # (batch, time, emb)\n",
    "        return out\n",
    "\n",
    "class BaselineWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.attn = SelfAttention()\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time)\n",
    "        x = self.emb(x)              # (batch, time, emb)\n",
    "        x = self.attn(x)             # (batch, time, emb)\n",
    "        x = x.max(dim=1).values      # global max pooling over time\n",
    "        logits = self.classifier(x)  # (batch, num_classes)\n",
    "        return logits\n"
   ],
   "id": "6e9e7a03e5ea4cd",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 5",
   "id": "5e957471752c8287"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:55:57.638250Z",
     "start_time": "2025-11-30T16:55:57.633958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttnSelectModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.attn = SelfAttention()          # from Q4\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)                      # (batch, time, emb)\n",
    "        x = self.attn(x)                     # (batch, time, emb)\n",
    "\n",
    "        # SELECT POOLING: take first position only\n",
    "        x = x[:, 0, :]                       # (batch, emb)\n",
    "\n",
    "        logits = self.classifier(x)          # (batch, num_classes)\n",
    "        return logits"
   ],
   "id": "6f767434340ae276",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T17:16:15.915112Z",
     "start_time": "2025-11-30T17:16:15.912702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment_q5(dataset_name, x_train, y_train, x_val, y_val, w2i, num_classes,\n",
    "                   pool_type=\"mean\", emb_dim=300, batch_size=256, epochs=5, lr=1e-3):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    vocab_size = len(w2i)\n",
    "\n",
    "    model = BaselineWithAttention(\n",
    "        vocab_size=vocab_size,\n",
    "        emb_dim=emb_dim,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    x_train_t, y_train_t, x_val_t, y_val_t = prepare_data(x_train, y_train, x_val, y_val, w2i)\n",
    "    train_loader, val_loader = make_loaders(x_train_t, y_train_t, x_val_t, y_val_t, batch_size)\n",
    "\n",
    "    print(f\"\\n=== {dataset_name.upper()} — Pool: {pool_type} ===\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "217c497419cb2fd6",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T17:17:32.119357Z",
     "start_time": "2025-11-30T17:16:16.464838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train on imdb\n",
    "MAX_LEN = 256\n",
    "\n",
    "(x_tr1, y_tr1), (x_va1, y_va1), (i2w1, w2i1), num_cls1 = load_imdb()\n",
    "x_tr1 = [seq[:MAX_LEN] for seq in x_tr1]\n",
    "x_va1   = [seq[:MAX_LEN] for seq in x_va1]\n",
    "\n",
    "run_experiment_q5(\n",
    "    \"imdb_attn_select\",\n",
    "    x_tr1, y_tr1,\n",
    "    x_va1, y_va1,\n",
    "    w2i1, num_cls1,\n",
    "    emb_dim=300,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    lr=1e-2\n",
    ")"
   ],
   "id": "eece8e9b2c39e49e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IMDB_ATTN_SELECT — Pool: mean ===\n",
      "Epoch 1: train_loss=1.1021, val_acc=0.5554\n",
      "Epoch 2: train_loss=0.4529, val_acc=0.8468\n",
      "Epoch 3: train_loss=0.2644, val_acc=0.8688\n",
      "Epoch 4: train_loss=0.1708, val_acc=0.8662\n",
      "Epoch 5: train_loss=0.1118, val_acc=0.8800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaselineWithAttention(\n",
       "  (emb): Embedding(99430, 300)\n",
       "  (attn): SelfAttention()\n",
       "  (classifier): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T17:18:06.776824Z",
     "start_time": "2025-11-30T17:18:00.618952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train on imdb synth\n",
    "(x_tr2, y_tr2), (x_va2, y_va2), (i2w2, w2i2), num_cls2 = load_imdb_synth()\n",
    "run_experiment_q5(\n",
    "    \"imdb_synth_attn_select\",\n",
    "    x_tr2, y_tr2,\n",
    "    x_va2, y_va2,\n",
    "    w2i2, num_cls2,\n",
    "    emb_dim=300,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    lr=1e-3\n",
    ")"
   ],
   "id": "e64e3d4c23816c9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IMDB_SYNTH_ATTN_SELECT — Pool: mean ===\n",
      "Epoch 1: train_loss=0.2466, val_acc=1.0000\n",
      "Epoch 2: train_loss=0.0139, val_acc=1.0000\n",
      "Epoch 3: train_loss=0.0034, val_acc=1.0000\n",
      "Epoch 4: train_loss=0.0014, val_acc=1.0000\n",
      "Epoch 5: train_loss=0.0007, val_acc=1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaselineWithAttention(\n",
       "  (emb): Embedding(69, 300)\n",
       "  (attn): SelfAttention()\n",
       "  (classifier): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T17:18:25.481109Z",
     "start_time": "2025-11-30T17:18:21.778421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train on xor\n",
    "(x_tr3, y_tr3), (x_va3, y_va3), (i2w3, w2i3), num_cls3 = load_xor()\n",
    "run_experiment_q5(\n",
    "    \"xor_attn_select\",\n",
    "    x_tr3, y_tr3,\n",
    "    x_va3, y_va3,\n",
    "    w2i3, num_cls3,\n",
    "    emb_dim=300,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    lr=1e-3\n",
    ")"
   ],
   "id": "c49176912326c932",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XOR_ATTN_SELECT — Pool: mean ===\n",
      "Epoch 1: train_loss=0.0126, val_acc=1.0000\n",
      "Epoch 2: train_loss=0.0002, val_acc=1.0000\n",
      "Epoch 3: train_loss=0.0001, val_acc=1.0000\n",
      "Epoch 4: train_loss=0.0000, val_acc=1.0000\n",
      "Epoch 5: train_loss=0.0000, val_acc=1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaselineWithAttention(\n",
       "  (emb): Embedding(6, 300)\n",
       "  (attn): SelfAttention()\n",
       "  (classifier): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bea8983aa5841f9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
